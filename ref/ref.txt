大语言模型
定义与核心思想
大语言模型（英语：Large Language Model，简称LLM）是指使用大量文本数据训练的深度学习模型，使得该模型可以生成自然语言文本或理解语言文本的含义。这些模型可以通过在庞大的数据集上进行训练来提供有关各种主题的深入知识和语言生产。其核心思想是通过大规模的无监督训练学习自然语言的模式和结构，在一定程度上模拟人类的语言认知和生成过程。
应用场景
LLM在多种应用场景下表现出色，不仅能执行拼写检查和语法修正等简单的语言任务，还能处理文本摘要、机器翻译、情感分析、对话生成和内容推荐等复杂任务。通过在大规模数据集上进行预训练，大语言模型获得了强大的通用建模能力和泛化能力。近期，GPT-4和LLaMA等大语言模型在自然语言处理等领域取得了巨大的成功，并逐步应用于金融、医疗和教育等特定领域。
重要事件
2023年12月26日，大语言模型入选“2023年度十大科技名词”。2024年4月，在第27届联合国科技大会上，世界数字技术院发布了《生成式人工智能应用安全测试标准》和《大语言模型安全测试方法》两项国际标准，由OpenAI、蚂蚁集团、科大讯飞、谷歌、微软、英伟达、百度、腾讯等数十家单位多名专家学者共同编制而成。
技术原理
大语言模型是一种基于深度学习的人工智能技术，也是自然语言处理的核心研究内容之一。其核心是使用大规模数据集对模型进行训练，从而使其能够生成自然语言文本或理解语言文本的含义。这些模型通过层叠的神经网络结构，学习并模拟人类语言的复杂规律，达到接近人类水平的文本生成能力。大语言模型采用与小模型类似的Transformer架构和预训练目标（如 Language Modeling），与小模型的主要区别在于增加模型大小、训练数据和计算资源。相比传统的自然语言处理（Natural Language Processing, NLP）模型，大语言模型能够更好地理解和生成自然文本，同时表现出一定的逻辑思维和推理能力。
发展历史
技术起源
大语言模型的起源可以追溯到20世纪50年代，当时人工智能领域的先驱们开始探索如何让计算机理解和生成人类语言。20世纪70年代由贾里尼克提出的N-gram语言模型是最常用的统计语言模型之一，广泛用于当今的多种自然语言处理系统中。N-gram模型将文本序列划分为长度为N的连续词组（N-gram），并利用大量语料库训练模型，以预测给定N-gram的后续词。N-gram模型虽然是一种有效的语言建模技术，但是存在着一些局限性，如数据稀疏性、计算复杂性和语言模型的可扩展性等。基于N-gram语言模型的不足，人们开始尝试用神经网络来建立语言模型。
发展历程
雏形阶段
20世纪40年代末和50年代开始采用计算机技术来研究和处理自然语言。1950年，图灵测试诞生。1954年，美国人乔治·戴沃尔设计出第一台可编程机器人。1956年，美国达特茅斯学院举行历史上第一次人工智能研讨会，标志人工智能诞生。
1966年，世界上第一个聊天机器人--ELIZA，由美国麻省理工学院（MIT）约瑟夫·魏岑鲍姆发布。ELIZA能通过脚本理解简单的自然语言，并能产生类似人类的互动。
1975年，Frederick Jelinek等人在论文《Continuous Speech Recognition by Statistical Methods》中提出并应用N-gram模型于语音识别任务。之后随着神经网络的发展，出现了神经语言模型。
2010年，斯坦福大学推出Core NLP套件，该套件提供了一套工具和算法，帮助研究人员处理复杂的NLP任务，允许开发人员执行情感分析和命名实体识别。
2011年，出现了一个较小版本的Google Brain，具有单词嵌入等高级功能，使自然语言处理系统能够更清楚地理解上下文。
2013年，自然语言处理模型Word2Vec诞生，首次提出将单词转换为向量的“词向量模型”，以便计算机更好理解和处理文本数据。
GPT模型问世
2017年，Google发布论文《Attention is all you need》，提出Attention机制和基于此机制的Transformer架构。此架构价值在于是一种完全基于注意力机制的序列转换模型，而不依赖循环神经网络（Recurrent Neural Network, RNN）、卷积神经网络（Convolutional Neural Network, CNN）或者长短期记忆（Long Short-Term Memory, LSTM）。
2018年，Google AI研究院的Jacob Devlin等人提出了BERT（Bidirectional Encoder Representation from Transformers）， BERT利用掩码机制构造基于上下文预测中间词的预训练任务，很大程度上提高自然语言处理任务的性能。BERT出现具有重大意义，尤其是预训练+参数微调”的研究范式，此后出现更多预训练语言模型都是以该范式为基础；同年，OpenAI公司同样发布了自己的模型GPT（Generative Pre-Training），这是一个典型的生成式预训练模型。
2019年，OpenAI发布GPT-2，该模型可以不用根据下游任务数据进行参数优化，可以根据给定指令自行理解并完成任务。
2020年，OpenAI发布GPT-3，并在Github上开源GPT-3部分样本和数据集。该模型拥有1750亿个参数。该模型的发布是一件跨时代的事情，意味着自然语言处理领域的大语言模型真正意义上出现了，从此正式开启大语言模型时代。
进阶突破阶段
2019年，Radford等人使用GPT-2模型研究大语言模型在零样本情况下的任务处理能力；Brown等人在GPT-3模型上研究通过语境学习进行少样本学习的方法指令微调将大量各类型任务，统一为生成式自然语言理解框架，并构造训练语料进行微调。
2022年，Ouyang等人提出使用“有监督微调+ 强化学习”的InstructGPT算法。这些方法逐渐扩展到利用生成式框架针对大量任务进行有监督微调的方法，有效提升模型的性能。
2022年11月30日，OpenAI公司发布ChatGPT，该模型属于一类基于GPT技术的大语言模型。Google、Microsoft、NVIDIA等公司也给出了自己的大语言模型。
2023年，谷歌公布聊天机器人Bard，它由谷歌的大语言模型LaMDA驱动；同年，百度正式宣布将推出文心一言，3月16日正式上线。文心一言的底层技术基础为文心大模型，底层逻辑是通过百度智能云提供服务，吸引企业和机构客户使用API和基础设施，共同搭建AI模型、开发应用，实现产业AI普惠；3月，Open AI发布多模态预训练大模型GPT4.0。
2023年4月13日，亚马逊云服务部门在官方博客宣布推出Bedrock生成式人工智能服务，以及自有的大语言模型泰坦（Titan）。
2024年3月，Databricks推出大语言模型DBRX，号称“现阶段最强开源AI”；马斯克的xAI公司正式发布大模型Grok-1，参数量达到3140亿，超OpenAI GPT-3.5的1750亿；4月，在瑞士举行的第27届联合国科技大会上，世界数字技术院（WDTA）发布了《生成式人工智能应用安全测试标准》和《大语言模型安全测试方法》两项国际标准，是由OpenAI、蚂蚁集团、科大讯飞、谷歌、微软、英伟达、百度、腾讯等数十家单位的多名专家学者共同编制而成。
重大节点
Transformer结构
在大语言模型的发展历程中，最重要的里程碑是2018年谷歌发布的Transformer模型，它采用了自注意力机制，可以更好地捕捉语言中地长距离依赖关系，从而极大地提高了大语言模型的效果。通过其自注意力机制，Transformer不仅解决了递归神经网络在并行化处理上的限制，还显著提升了模型处理大规模数据集的能力。这种技术的进步为预训练语言模型（PLMs）的发展铺平了道路，使得这些模型能够更加灵活地适应各种不同的下游任务。
Transformer是一种用于序列到序列（Sequence-to-Sequence）任务的神经网络模型，如机器翻译、语音识别和生成对话等。它是第一个完全依赖于自注意力机制来计算其输入和输出的表示的转换模型。序列到序列模型采用的是编码器-解码器结构，编码器-解码器结构采用堆叠的多头注意力机制加全连接层。通过查询-键-值的模式使用多头注意力。由于Transformer模型中既没有递归，也没有卷积，如果需要获得输入序列精准的位置信息，必须插入位置编码。位置编码和输入嵌入有相同的维度，所以二者可以实现相加运算，位置编码方式可以有多种。
从人类反馈中强化学习（RLHF）
人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）是一种利用人工指导来微调预先训练好的大型语言模型（LLMs）的方法。由三个相互关联的过程组成：反馈收集、奖励建模和策略优化。RLHF优势在于能更好地与人类的意图保持一致，以及以未来的反馈为条件进行规划，从各种类型的反馈中进行流畅的学习，并根据需要对反馈进行整理。此外，RLHF还允许机器通过抽象人类的价值学习，并不是简单地模仿人类的行为。
2023 年4月OpenAI联合创始人John Schulman在Berkeley EECS会议上所做的报告“ReinforcementLearning from Human Feedback：Progress and Challenges”，分享了OpenAI在人类反馈的强化学习方面的进展，分析监督学习和强化学习各自存在的挑战。基于上述报告及相关讨论，强化学习在大语言模型上的重要作用可以概括为以下几个方面。
一，强化学习与有监督学习相比，更有可能从整体层面去考虑影响。这是因为二者在反馈粒度方面存在差异，强化学习不仅能够兼顾表达多样性，还能增强对微小变化的敏感性，所以它相对而言更契合大语言模型。而且，强化学习还允许模型呈现出不同的多样性表达。
二，强化学习更容易解决幻觉问题。有监督学习算法非常容易使得求知型查询产生幻觉。在模型并不包含或者不知道答案的情况下，有监督训练仍然会促使模型给出答案。而使用强化学习方法，则可以通过定制奖励函数，将正确答案赋予非常高的分数，将放弃回答的答案赋予中低分数，将不正确的答案赋予非常高的负分，使得模型学会依赖内部知识选择放弃回答，从而在一定程度上缓解模型的幻觉问题。
三，强化学习可以更好地解决多轮对话奖励累积问题。多轮对话能力是大语言模型重要的基础能力之一。多轮对话是否达成最终目标，需要考虑多次交互过程的整体情况，因此很难使用有监督学习的方法构建。而使用强化学习方法，可以通过构建奖励函数，根据整个对话的背景及连贯性对当前模型输出的优劣进行判断。
专家混合模型
GPT-4 采用了专家混合模型（Mixture of Experts，MoE）架构，总共有1.8 万亿个参数。GPT-4使用了16 个专家，每个专家的参数约为1110亿，每次前向传递使用2 个专家进行路由，同时还有550 亿个共享参数用于注意力机制。MoE 架构在减少推理所需的参数量的同时，仍然可以使用更大规模的模型参数。
混合专家系统类思路是大模型落地比较优质的路径。
提示学习
提示学习（Prompt-based Learning）不同于传统的监督学习，它直接利用了在大量原始文本上进行预训练的语言模型，并通过定义一个新的提示函数，使该模型能够执行小样本甚至零样本学习，以适应仅有少量标注或没有标注数据的新场景。
实现自我复制
2025年2月11日消息，据最新研究显示，人工智能（AI）可能已经跨越了一个关键的“红线”—— 实现了自我复制。2024 年 12 月 9 日，复旦大学的研究人员在预印本数据库 arXiv 上发表了一项研究，指出两种流行的大型语言模型（LLMs）能够在无人类干预的情况下克隆自身。
基本原理
训练流程
预训练
预训练是大语言模型训练的首要步骤，其目标在于使模型掌握语言的统计模式与语义信息。主流的预训练阶段流程大致相同，其中关键要素是数据，需收集海量无标注数据，像互联网上的文本、新闻、博客、论坛等。这些数据可以涵盖多种语言，且要经过一定的清理和处置，去除噪声、无关信息以及涉及个人隐私的内容，最后以tokenizer粒度输入到前述的语言模型中。经清洗处理后的这些数据用于训练和优化语言模型。在预训练过程中，模型会习得词汇、句法和语义的规律以及上下文的关系。
在预训练语料集方面，GPT-3中通过主要包含经过过滤的Common Crawl数据集、WebText2、Books1、Books2以及英文Wikipedia等数据集合。其中Common Crawl的原始数据有45TB，进行过滤后仅保留了570GB的数据。通过子词方式对上述语料进行切分，大约一共包含5000亿子词。为了保证模型使用更多高质量数据进行训练，在GPT-3训练时，根据语料来源的不同，设置不同的采样权重。在完成3000亿子词训练时，英文Wikipedia的语料平均训练轮数为3.4次，而Common Crawl和Books2仅有0.44次和0.43次。
由于Common Crawl数据集合的过滤过程繁琐复杂，OPT则采用了混合RoBERTa、Pile和Pushshift.io Redit数据的方法。由于这些数据集合中包含的绝大部分都是英文数据，因此OPT也从Common Crawl数据集中抽取了部分非英文数据加入训练语料。
BigScience大型开放科学开放获取多语言模型（BigScience Large Open-science Open-access Mul-tilingual Language Model, BLOOM）运用Megatron-DeepSpeed 框架进行训练，主要包括两个部分：Megatron-LM 提供张量并行能力和数据加载原语；DeepSpeed 提供 ZeRO 优化器、模型流水线以及常规的分布式训练组件。通过这种方式能够实现数据、张量和流水线的三维并行。
数据收集
预训练语料有两种来源：
1.	通用语料：如网页、书籍和会话文本等，可以增强大语言模型的语言建模和泛化能力。
2.	专业语料：有研究将预训练语料库扩展到更专业的数据集，如多语言数据、科学数据和代码，赋予大语言模型特定的任务解决能力。
数据收集完后需要对这些数据进行预处理，包括去噪、去冗余、去除不相关和潜在有毒的数据。
基础大模型训练
由于模型参数量和所使用的数据量巨大，所以普通服务器单机无法完成训练过程，因此通常采用分布式架构完成训练。
指令微调
在完成预训练后，就可以通过指令微调去挖掘和增强语言模型本身具备的能力，这步也是很多企业以及科研研究人员利用大模型的重要步骤。
Instruction tuning（指令微调）是大模型微调的一种具体方式，它是有监督微调（Supervised Fine-Tuning, SFT）的一种特殊形式，旨在让模型理解和遵循人类指令。在指令微调阶段，首先需要准备一系列的NLP任务，并将每个任务转化为指令形式，其中指令包括人类对模型应该执行的任务描述和期望的输出结果。然后，使用这些指令对已经预训练好的大语言模型进行监督学习，使得模型通过学习和适应指令来提高其在特定任务上的表现。
通过指令微调，大模型学习到了如何响应人类指令，可以根据指令直接能够生成合理的答案。
为了让模型训练更加高效和简单，这个阶段还有一种高效的fine-tuning技术，这为普通的从业者打开了通向使用大模型的捷径。
大模型高效微调（Parameter-Efficient Fine-Tuning, PEFT）旨在通过最小化微调参数的数量和计算复杂度，达到高效的迁移学习的目的，提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。在训练过程中，预训练模型的参数保持不变，只需微调少量的额外参数，就可以达到与全量微调相当的性能。
很多研究对PEFT方法进行了探索，例如Adapter Tuning和Prefix Tuning等。其中，Adapter Tuning方法在面对特定的下游任务时，将预训练模型中的某些层固定，只微调接近下游任务的几层参数。而Prefix Tuning方法则是在预训练模型的基础上，添加一些额外的参数，这些参数在训练过程中会根据特定的任务进行更新和调整。
工业界常用的Adapter Tuning的技术是Low-Rank Adaptation（LoRA）。它通过最小化微调参数的数量和计算复杂度，实现高效的迁移学习，以提高预训练模型在新任务上的性能。LoRA 的核心思想是将预训练模型的权重矩阵分解为两个低秩矩阵的乘积。通过这种分解，可以显著减少微调参数的数量，并降低计算复杂度。该方式和机器学习中经典的降维的思想很类似，类似地，LoRA 使用了矩阵分解技术中的奇异值分解 (Singular Value Decomposition, SVD) 或低秩近似 (Low-Rank Approximation) 方法，将原始权重矩阵分解为两个低秩矩阵的乘积。
在微调过程中，LoRA 只更新这两个低秩矩阵的参数，而保持其他预训练参数固定不变。这样可以显著减少微调所需的计算资源和时间，并且在很多任务上取得了与全量微调相当的性能。
LoRA技术的引入使得在大规模预训练模型上进行微调更加高效和可行，为实际应用提供了更多可能性。
类人对齐
由于模型输出的结果与人类回答差距很大，因此需要进一步优化模型，使模型的输出与人类习惯对齐。其中OpenAI开发ChatGPT的人类反馈强化学习是最具代表性也是最成功的。
奖励建模
奖励建模（Reward Modeling）阶段的目标是构建一个文本质量对比模型，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序。奖励模型（RM模型）可以通过二分类模型，对输入的两个结果之间的优劣进行判断。RM模型与基础语言模型和SFT模型不同，RM模型本身并不能单独提供给用户使用。
奖励模型的训练通常和SFT模型一样，使用数十块GPU，通过几天时间完成训练。由于RM模型的准确率对强化学习阶段的效果有至关重要的影响，因此通常需要大规模的训练数据对该模型进行训练。
强化学习
强化学习（Reinforcement Learning）阶段根据数十万用户给出的提示词，利用前一阶段训练的RM模型，给出SFT模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果。
使用强化学习，在SFT模型基础上调整参数，使得最终生成的文本可以获得更高的奖励（Reward）。该阶段需要的计算量相较预训练阶段也少很多，通常仅需要数十块GPU，数天即可完成训练。
Andrej Karpathy也指出，强化学习并不是没有问题的，它会使基础模型的熵降低，从而减少了模型输出的多样性。经过强化学习方法训练后的RL模型，就是最终提供给用户使用、具有理解用户指令和上下文的类ChatGPT 系统。由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加RM模型的准确率问题，使得在大语言模型上有效应用强化学习非常困难。
工作原理
大语言模型的工作原理基于深度学习架构。它首先会收集海量的文本数据，之后通过词向量表将单词映射到特定的向量空间以便计算机以数值化方式处理语言，随后利用大量的计算资源对具有庞大数量参数的神经网络模型进行训练。通过在训练过程中不断地调整模型参数，让模型去学习数据中的语言模式、语义信息等，使得模型能够在各类自然语言处理任务中取得最佳表现。
大语言模型的“大”主要体现几个方面在：一是参数数量庞大；二是训练数据量大；三是对计算资源需求高。正是因为具备这些“大”的特点，很多先进的大语言模型参数不断增多，泛化性能愈发出色，在各种专门的领域输出结果也越来越准确。
模型特点
训练成本
大语言模型的训练成本非常高，通常达到数百万美元甚至更多。例如，OpenAI的GPT-4模型训练成本超过1亿美元。随着模型规模的增大，训练成本急剧上升，2023年发布的模型训练成本已逼近2亿美元。预计到2024年底或2025年初，新一代模型的训练成本可能逼近10亿美元。这些成本包括了数据准备、硬件成本、模型架构设计和优化等多个方面。
局限性
不能创造语言
大模型至多是会使用语言，而远谈不上能创造语言、发明语言。大语言模型的基础仍然是深度学习技术，即利用大量的文本数据来训练模型，只不过模型的参数规模更为庞大，但与产生语言的劳动、实践根本不沾边。
不能深度理解人类
大语言模型只是人类生存实践的旁观者和应答者，缺乏共情能力，还达不到像人类理解那样的深刻性与丰富性，而深层理解更彰显人类智能的特殊性。
不能全面嵌入社会
以ChatGPT为代表的大语言模型仍然不能像人一样在社会中进行交往与实践，不能以人类体悟语境的方式来体悟语境，因此，谈论ChatGPT拥有媲美人类的智能，完全理解人类的语言，还为时尚早。
安全性不高
安全性是大型语言模型必须直面的关键问题之一。大型语言模型可以在众多学科领域的任务中得以应用，然而，这也表明此类模型会遭遇广泛的内容安全难题。尽管大型语言模型已借助基于人类反馈的强化学习等诸多方式，努力使模型输出与人类价值观相契合，但在应用于各个领域时，语言模型依旧容易遭到恶意利用，进而生成诸如偏见言论、煽动性话语、隐私侵犯言论等存在安全隐患的文本。
成本高昂
大语言模型在训练和部署过程中，会耗费大量的计算资源与人力资源，成本高昂。对部分中小型企业来说，很难承受这样的成本，也难以获取充足的技术支持及资源。在企业级应用方面，采用百亿级基础模型较为适宜，再依据不同需求去训练相应的垂直模型，如此只需承担垂直训练的成本。不过，企业怎样实现高效的垂直训练以及如何把控成本，依旧是大模型需要面对的问题之一。
不能保障内容可信
可信度当前是大型语言模型的重大局限之一。虽然大语言模型能够用于处理各种真实场景中的问题，然而它依旧会产出不可信的文本。现今使用者只能按照自身需求去核验生成的内容是否真实可靠，很难具备权威说服力。与此同时，模型在解决涉及推理的问题时，有可能由于推理过程出现错误而得到不可信的结果。这对其研究发展以及应用落地都有着负面的影响。
相关研究与发展
相关社会影响
年度词汇
2023年12月6日，大语言模型入选国家语言资源监测与研究中心发布的“2023年度中国媒体十大流行语”。
2023年12月26日，大语言模型入选“2023年度十大科技名词”。
科技发展
大语言模型的快速进步，正在激发新业态、新模式，由此带来的工作方式、教育模式等的变革。它不仅是一项技术，更是未来国力竞争与生产力提高的重要资源。以深度学习平台和大模型为代表的AI新型基础设施，对科技创新、产业升级和高质量发展意义重大。
翻译服务
大语言模型快速发展背后是自然语言处理（NLP）技术的突破，结合利用深度学习和大规模数据训练，大语言模型不仅能够理解和生成更为精准和流畅的翻译，且能处理多种语言间的微妙差异，还能进行上下文理解，使得翻译效果更加自然、智能。
最新研究进展
星火大模型
讯飞星火认知大模型是科大讯飞发布的语言大模型。该模型于2023年5月首次发布，后续经过多次升级。2023年10月，讯飞发布了讯飞星火认知大模型V3.0。2024年1月，讯飞发布了讯飞星火认知大模型V3.5。
2024年10月，讯飞星火4.0 Turbo在第七届世界声博会暨2024科大讯飞全球1024开发者节上被正式发布，该模型七大核心能力全面超过GPT-4 Turbo，数学和代码能力超越GPT-4o，国内外中英文14项主流测试集中讯飞星火4.0 Turbo在9项测试集中实现超越。
GPT-4
2023年3月发布的 GPT-4将文本输入扩展到多模态信号。2024年5月14日，新一代旗舰生成模型 GPT-4o 正式发布。GPT-4o 具备了对文本、语音、图像三种模态的深度理解能力，反应迅速且富有情感色彩，极具人性化。OpenAI官网介绍，GPT-4o中的o代表意为全能的前缀omni，称它向更自然的人机交互迈进了一步，因为它接受文本、音频和图像的任意组合作为输入内容，并生成文本、音频和图像的任意组合输出内容。
AI大模型
2025年1月8日，在2025年国际消费电子展的高通展台，一台白色等人高的人形机器人用流利的英语热情问候走近的参观者们。这台人形机器人名为“通天晓”，是全球首台完全基于高通SoC的端侧多模态AI大模型人形机器人。这款基于端侧大模型的人形机器人为具身智能产业的创新发展开辟了更优路径。通过阿加犀技术成功部署的端侧大模型，让机器人的‘大脑’显著‘进化’，其多模态处理能力结合视觉、听觉、触觉等各种输入，提升了机器人对复杂场景的理解，从而极大增强了机器人的通用性和泛化性。
未来发展方向
多模态大语言模型
随着输入数据源模态的扩展，多模态大模型的构建思路通样按照网络架构的不同，可以分为基于理解模型的范式、基于生成式模型的范式，以及基于编解码的模型构建方法。ChatGPT 提供了一个跨领域的具有卓越会话能力和推理能力的语言界面。然而，由于ChatGPT是一个语言模型，无法处理、生成来自视觉世界的图像。同时，视觉基础模型（Visual Foundation Model，VFM），如视觉变换器或 Stable Diffusion，虽然显示出强大的视觉理解和生成能力，但只是具有一轮固定输入和输出的特定任务的专家。如何将 ChatGPT 的上下文交互能力同视觉、语音数据分析能力进行有效整合，将为多模态大模型训练提供新的思路。
轻量化大语言模型
随着技术的发展，可以预见未来的生成式人工智能模型的规模将继续增长。更大规模的模型可以提供更深入、更准确的语言理解和生成能力，使得对话更加自然流畅，并且使模型能够更好地理解和回复复杂的问题和指令。然而，这些模型参数规模与训练数据规模的迅速增长带来极大的成本，为现实应用中的存储、分发、部署等带来了挑战。因此，需要对生成式人工智能模型进行轻量化和优化，以提高模型的效率与实用性。总之，更轻量化和高效的生成式人工智能模型将有助于其在更广泛的应用场景中发挥更大的作用。
类脑化认知
类脑化是指生成式人工智能应具有与人类大脑类似的特性和能力，以更好地模拟人类的认知和学习过程。现有的生成式模型的训练方式与人类知识获取的方式存在很大的差异，大模型的生成式过程属于快思考，是一种直觉思维，容易出现错误和偏见，且不适合规划类任务。而人类的思维方式是慢思考，是一种理性思维。因此，未来的生成式人工智能需要更复杂和多样化的神经元系统，以及更加灵活的神经网络连接方式，从而模拟人类神经元与脑区的各种特性和行为。基于更强的类脑化认知，生成式人工智能可能将在科学智能领域发挥更大的作用，即学习、模拟和预测自然界和人类社会的各种现象和规律，从而推动科学发现和创新。
应用领域
教育
在线讨论与反思学习场景：赋能高阶思维能力培养
在线讨论与反思学习场景中的文本数据在一定程度上反映学生在线学习过程中的认知和情感表现。具有自然语言理解优势的BERT可对学生文本数据中的认知与情感进行识别，为赋能学生高阶思维能力培养奠定基础。同时探究学生在线学习认知和情感发展规律。
人机协同提问场景：加强阅读理解能力
自我提问可以促进学习专注度，加深对阅读内容的理解，但当前学生提问普遍存在水平不高、类型单一等问题。对此，可以利用T5（2019年谷歌提出的一种基于Transformer架构的自然语言处理模型）和GPT系列的自然语言生成优势，为高质量问题创建提供支持，进而加强学生的阅读理解能力。利用GPT-3自动生成提示语（包括提问类型、答案、提问视角），通过多轮人机对话，帮助学生提出深层次问题。GPT-3更能促使小学生提出一系列与知识点相关的、深层次的问题，以加强深度阅读理解。总的来说，大语言模型可以利用其文本生成优势，通过人机协同对话形式辅助学生提问，进而提升其阅读理解能力。
人机协同写作和数学解题场景：提升写作和解题水平
写作与数学解题逻辑教学作为学科教学领域的两项重难点，一直存在学生写作时“不愿写”“没得写”“不会写”和数学解题答题不规范、传统教学指导效率低等问题。对此，GPT系列或类T5结构模型因其内容创作和数学推理优势，可以广泛应用于智能写作工具研究和数学解题辅助研究领域，进而有效提升学生的写作和数学解题水平。
金融
金融行业需要处理海量文本信息，大语言模型有助于分析和提取新闻媒体、研究报告、财务报表、企业公告、政府政策等文本信息中的价值。同时，金融信息具有强时效性，大语言模型可以做出秒级分析并提出建议。对于负债业务，基于大语言模型的智能客服可以协助优化存款业务流程，同时节省人力成本，提升服务效率。
政务
随着中国推动人工智能技术研究及其在政务领域的应用，大语言模型在政务领域发挥了巨大的作用，包括政务文本分类、政务问答、政务命名实体识别、舆情风险识别和政务关系抽取，但同时政务大语言模型研究仍处在探索阶段，存在许多需要解决的问题，即数据多模态化、正确面对“模型即服务”趋势、注重数据高安全性、明确责任边界。
医疗
在医疗行业，大语言模型的应用正在开启一场盖临床诊断、治疗护理、医学研究等方面的技术革命。在临床诊断方面,将大语言模型用于对患者病历、检查报告和生理参数等进行深入的自然语言处理和整公医疗大数据的分析，辅助医生快速准确地诊断疾病并制定治疗方案。
在治疗护理方面，Google 的 Med-PaLM 2和EPFL( École polytechnique Fédérale de Lausanne)的Meditron 等工具展示了在提高临床效率和患者护理水平方面的应用潜力。
在医学研究方面，将多模态大语言模型用于扫描和综合分析海量的科学论文，识别潜在的药物靶点和功效因素，加速了新药的研发。
在蛋白质设计和新药发现方面，大语言模型已经展示出蛋白质序列处理和结构预测的强大能力，极大地提升了蛋白质设计的效率和准确性。
办公
在2024年世界人工智能大会上，金山办公发布WPS AI 2.0，并推出政务自研模型——金山政务办公模型1.0。WPS AI是金山办公旗下基于大语言模型的人工智能办公助手。WPS AI演示了升级后为个人用户新增的4个AI办公助手，分别是AI写作助手、AI阅读助手、AI数据助手、AI设计助手。
快手在大会期间正式推出视频生成大模型可灵网页端。同时，可灵推出更加清晰的高画质版、首尾帧控制、镜头控制等新功能，创作者单次生成的文生视频时长增加至10秒。
客户联络
提升自动回复能力
可以根据用户输入的问题提供快速和准确的响应，快速解决问题，节省客服团队大量的时间和资源，提高客户体验和满意度。
强化意图识别能力
观察客户联络领域所处现状，大部分是把简单、重复、流程性的问题，交给机器人处理；复杂的、需要情感关怀的问题，交由人工客服处理。而传统的智能客服在意图理解方面的能力，仍然相对薄弱。借助大模型，智能客服能够有效结合用户的历史对话、当前沟通内容等上下文语境，更精准地识别出用户的需求和意图。
优化人机交互体验
以ChatGPT为例来看，大模型的深度应用开创了客户使用体验的新范本。丰富的参数性参数和强大的内容生成能力，能够支持智能客服实现更加个性化的问答回复，而非过往千篇一律的机械式问答。
ChatGPT的应用已经有相对确定的场景，如扮演人工客服与客户沟通专业知识、提供专业的问答知识建议、对沟通记录进行质检标记、主动分析座席工作行为、发起产品推介、闲聊寒暄以及更“人性化”的引导留资等。
风险与挑战
可信性
大模型的可信性无法保障。尽管其生成的内容符合语言规则，通顺流畅且与人类偏好对齐，但在事实性、时效性和数据准确性方面存在较多问题，缺乏可信性评估能力，这使其生成的内容具有一定的欺骗性。
可解释性
大模型的可解释性较差。作为基于深度神经网络的“黑盒”模型，大模型的能力来源仍未被完全理解。例如，其涌现能力、规模定律、知识表示、逻辑推理能力、泛化能力、情景学习能力等方面仍需进一步研究，以提供理论支持并推动大规模实际应用的落地。
应用成本较高
此外，大模型的应用成本较高。由于其参数规模和数据规模庞大，导致训练和推理的计算量大、功耗高、部署困难，同时也存在延迟问题。这些因素极大地限制了大模型的应用范围。提高推理速度和降低使用成本是推动大规模应用的关键。
能力迁移
在小数据环境下，大模型的能力迁移也面临挑战。尽管更大的模型和数据能带来更强的能力，但在特定领域的小数据环境中，大模型的泛化能力和鲁棒性表现有限。通过对数据进行有效筛选、标注，并结合细分领域的微调技术，可以提升大模型在复杂场景和小数据环境下的适用性和可靠性。
技术风险
除了技术挑战，大模型还存在一系列技术风险。大模型强大的自然语言生成能力与语音合成、图像视频生成等技术结合，可能产生难以辨别的虚假内容，从而被用于制造虚假信息、恶意引导行为、舆论攻击，甚至危害国家安全。与此同时，大模型还面临数据投毒攻击、对抗样本攻击、模型窃取攻击及后门攻击等安全风险，这些漏洞可能被攻击者利用，威胁整个应用生态的稳定性。
隐私保护
最后，大模型在隐私保护方面也存在隐患。由于训练过程中利用了海量的互联网数据，个人、企业甚至国家的隐私数据可能被编码进模型参数中。研究显示，通过一定提示可以从大模型中窃取隐私数据，这一问题需要引起广泛关注和重视。

